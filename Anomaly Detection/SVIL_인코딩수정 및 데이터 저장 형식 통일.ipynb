{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46cc801f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xec in position 70: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2152/741643111.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Convert the encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mconvert_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2152/741643111.py\u001b[0m in \u001b[0;36mconvert_encoding\u001b[1;34m(input_path, output_path, read_encoding, write_encoding)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cp949'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Read the CSV file with the existing encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Write the CSV file with the new encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xec in position 70: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_encoding(input_path, output_path, read_encoding='cp949', write_encoding='utf-8'):\n",
    "    # Read the CSV file with the existing encoding\n",
    "    df = pd.read_csv(input_path, encoding=read_encoding)\n",
    "\n",
    "    # Write the CSV file with the new encoding\n",
    "    df.to_csv(output_path, encoding=write_encoding, index=False)\n",
    "\n",
    "# Input file path\n",
    "input_path = 'C:/Users/c0206/Desktop/data/hanteochart_daily/20230601_hanteochart_daily.csv'\n",
    "\n",
    "# Output file path\n",
    "output_path = 'C:/Users/c0206/Desktop/data_encoding_fix/20230601_hanteochart_daily.csv'\n",
    "\n",
    "# Convert the encoding\n",
    "convert_encoding(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed5dd4b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'mbcs' codec can't encode characters in position 0--1: invalid character",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2152/2240238717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/c0206/Desktop/data/hanteochart_daily'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mconvert_all_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2152/2240238717.py\u001b[0m in \u001b[0;36mconvert_all_files\u001b[1;34m(directory, read_encoding, write_encoding)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0minput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0moutput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"converted_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mconvert_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Converted {filename}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2152/741643111.py\u001b[0m in \u001b[0;36mconvert_encoding\u001b[1;34m(input_path, output_path, read_encoding, write_encoding)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Write the CSV file with the new encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwrite_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Input file path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m             )\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         libwriters.write_csv_rows(\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\mbcs.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmbcs_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBufferedIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'mbcs' codec can't encode characters in position 0--1: invalid character"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def convert_all_files(directory, read_encoding='utf-8', write_encoding='ANSI'):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_path = os.path.join(directory, filename)\n",
    "            output_path = os.path.join(directory, \"converted_\" + filename)\n",
    "            convert_encoding(input_path, output_path, read_encoding, write_encoding)\n",
    "            print(f\"Converted {filename}\")\n",
    "\n",
    "directory = 'C:/Users/c0206/Desktop/data/hanteochart_daily'\n",
    "convert_all_files(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cecb1b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>sales</th>\n",
       "      <th>album_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CIX</td>\n",
       "      <td>'OK' Episode 2 : I'm OK: 미니앨범 6집</td>\n",
       "      <td>82,566</td>\n",
       "      <td>99,079.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BOYNEXTDOOR</td>\n",
       "      <td>WHO!</td>\n",
       "      <td>33,930</td>\n",
       "      <td>40,778.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AB6IX</td>\n",
       "      <td>THE FUTURE IS OURS : LOST: 미니앨범 7집</td>\n",
       "      <td>22,676</td>\n",
       "      <td>23,962.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ENHYPEN</td>\n",
       "      <td>DARK BLOOD</td>\n",
       "      <td>20,547</td>\n",
       "      <td>20,977.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>DREAMCATCHER</td>\n",
       "      <td>[Apocalypse : From us]: 미니앨범 8집</td>\n",
       "      <td>12,714</td>\n",
       "      <td>17,583.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        artist                               album   sales  \\\n",
       "0           0           CIX    'OK' Episode 2 : I'm OK: 미니앨범 6집  82,566   \n",
       "1           1   BOYNEXTDOOR                                WHO!  33,930   \n",
       "2           2         AB6IX  THE FUTURE IS OURS : LOST: 미니앨범 7집  22,676   \n",
       "3           3       ENHYPEN                          DARK BLOOD  20,547   \n",
       "4           4  DREAMCATCHER     [Apocalypse : From us]: 미니앨범 8집  12,714   \n",
       "\n",
       "  album_index  \n",
       "0   99,079.20  \n",
       "1   40,778.00  \n",
       "2   23,962.46  \n",
       "3   20,977.38  \n",
       "4   17,583.40  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the encoding using 'utf-8' for reading and 'utf-8-sig' for writing\n",
    "convert_encoding(input_path, output_path, read_encoding='utf-8', write_encoding='utf-8-sig')\n",
    "\n",
    "# Read the first few lines of the converted file to check if the Korean characters are displayed correctly\n",
    "converted_df_utf8_sig = pd.read_csv(output_path)\n",
    "converted_df_utf8_sig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2c8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230624_data\n",
      "20230627_pre1_data\n",
      "20230630_merge_data\n",
      "hanteochart_daily\n",
      "hanteochart_year\n",
      "melon_boy\n",
      "melon_girl\n",
      "pre_hanteochart_daily\n",
      "pre_tiktok\n",
      "pre_twitter_artist\n",
      "pre_youtube_girl_light\n",
      "tiktok_boys\n",
      "tiktok_girls\n",
      "twitter_artist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_encoding(input_path, output_path, read_encoding='utf-8', write_encoding='utf-8-sig'):\n",
    "    # Read the CSV file with the existing encoding\n",
    "    df = pd.read_csv(input_path, encoding=read_encoding)\n",
    "\n",
    "    # Write the CSV file with the new encoding\n",
    "    df.to_csv(output_path, encoding=write_encoding, index=False)\n",
    "\n",
    "def convert_all_files(input_directory, output_directory,read_encoding='utf-8', write_encoding='utf-8-sig'):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_path = os.path.join(input_directory, filename)\n",
    "            output_path = os.path.join(output_directory, filename)\n",
    "            convert_encoding(input_path, output_path, read_encoding, write_encoding)\n",
    "            print(f\"Converted {filename}\")\n",
    "\n",
    "def get_subdirectories(directory):\n",
    "    subdirectories = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    return subdirectories\n",
    "\n",
    "directory = 'C:/Users/c0206/Desktop/data' # 원하는 디렉토리 경로\n",
    "subdirectories = get_subdirectories(directory)\n",
    "\n",
    "\n",
    "for subdirectory in subdirectories:\n",
    "    print(subdirectory)\n",
    "    input_directory = 'C:/Users/c0206/Desktop/data/'+subdirectory+'/'\n",
    "    output_directory = 'C:/Users/c0206/Desktop/data_encoding_fix/'+subdirectory+'/'\n",
    "    convert_all_files(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a64089f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230624_data', '20230627_pre1_data', '20230630_merge_data', 'hanteochart_daily', 'hanteochart_year', 'melon_boy', 'melon_girl', 'pre_hanteochart_daily', 'pre_tiktok', 'pre_twitter_artist', 'pre_youtube_girl_light', 'tiktok_boys', 'tiktok_girls', 'twitter_artist']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_subdirectories(directory):\n",
    "    subdirectories = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    return subdirectories\n",
    "\n",
    "directory = 'C:/Users/c0206/Desktop/data' # 원하는 디렉토리 경로\n",
    "subdirectories = get_subdirectories(directory)\n",
    "\n",
    "print(subdirectories) # 하위 디렉토리 명 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a61ac4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230627_pre1_data\n",
      "Converted hanteochart_daily_20230627.csv\n",
      "Converted hanteo_year.csv\n",
      "Converted pre1_concerthall_20230627.csv\n",
      "Converted pre1_melon_20230627.csv\n",
      "Converted pre1_tiktok_20230627.csv\n",
      "Converted pre1_twitter_artist_20230627.csv\n",
      "20230624_data\n",
      "Converted circle_month.csv\n",
      "Converted concerthall_20230627.csv\n",
      "Converted hanteochart_daily_20230627.csv\n",
      "Converted hanteo_year.csv\n",
      "Converted melon_20230627.csv\n",
      "Converted tiktok_20230627.csv\n",
      "Converted twitter_artist_20230627.csv\n",
      "Converted youtube_girl_light_20230627.csv\n",
      "20230630_merge_data\n",
      "Converted clustering_20230731.csv\n",
      "Converted han_d+melon+twt+crt+han_fw_20230630.csv\n",
      "Converted han_d+melon+twt+crt_20230630.csv\n",
      "Converted han_d+melon+twt_20230630.csv\n",
      "Converted han_d+melon_20230630.csv\n",
      "Converted model_20230630.csv\n",
      "Converted model_20230705.csv\n",
      "Skipping C:/Users/c0206/Desktop/data\\20230630_merge_data\\result.csv due to encoding error.\n",
      "Converted result.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_encoding(input_path, output_path, read_encoding='utf-8', write_encoding='utf-8-sig'):\n",
    "    try:\n",
    "        df = pd.read_csv(input_path, encoding=read_encoding)\n",
    "\n",
    "        df.to_csv(output_path, encoding=write_encoding, index=False)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Skipping {input_path} due to encoding error.\")\n",
    "\n",
    "def convert_all_files(input_directory, output_directory, read_encoding='utf-8', write_encoding='utf-8-sig'):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_path = os.path.join(input_directory, filename)\n",
    "            output_path = os.path.join(output_directory, filename)\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                os.remove(output_path)\n",
    "\n",
    "            convert_encoding(input_path, output_path, read_encoding, write_encoding)\n",
    "            print(f\"Converted {filename}\")\n",
    "\n",
    "def get_subdirectories(directory):\n",
    "    subdirectories = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    return subdirectories\n",
    "\n",
    "\n",
    "directory = 'C:/Users/c0206/Desktop/data' #디렉토리 경로 변경 필요\n",
    "subdirectories = get_subdirectories(directory)\n",
    "\n",
    "subdirectories=[\"20230627_pre1_data\", \"20230624_data\", \"20230630_merge_data\"]\n",
    "\n",
    "for subdirectory in subdirectories:\n",
    "    print(subdirectory)\n",
    "    input_directory = os.path.join(directory, subdirectory)\n",
    "    output_directory = os.path.join('C:/Users/c0206/Desktop/data_encoding_fix', subdirectory)\n",
    "    convert_all_files(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac3c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dates in circle_month.csv\n",
      "Processed dates in concerthall_20230627.csv\n",
      "Processed dates in hanteochart_daily_20230627.csv\n",
      "Processed dates in hanteo_year.csv\n",
      "Processed dates in melon_20230627.csv\n",
      "Processed dates in tiktok_20230627.csv\n",
      "Processed dates in twitter_artist_20230627.csv\n",
      "Processed dates in youtube_girl_light_20230627.csv\n",
      "Processed dates in hanteochart_daily_20230627.csv\n",
      "Processed dates in hanteo_year.csv\n",
      "Processed dates in pre1_concerthall_20230627.csv\n",
      "Processed dates in pre1_melon_20230627.csv\n",
      "Processed dates in pre1_tiktok_20230627.csv\n",
      "Processed dates in pre1_twitter_artist_20230627.csv\n",
      "Processed dates in clustering_20230731.csv\n",
      "Processed dates in han_d+melon+twt+crt+han_fw_20230630.csv\n",
      "Processed dates in han_d+melon+twt+crt_20230630.csv\n",
      "Processed dates in han_d+melon+twt_20230630.csv\n",
      "Processed dates in han_d+melon_20230630.csv\n",
      "Processed dates in model_20230630.csv\n",
      "Processed dates in model_20230705.csv\n",
      "Processed dates in 20230601_hanteochart_daily.csv\n",
      "Processed dates in 20230602_hanteochart_daily.csv\n",
      "Processed dates in 20230603_hanteochart_daily.csv\n",
      "Processed dates in 20230604_hanteochart_daily.csv\n",
      "Processed dates in 20230605_hanteochart_daily.csv\n",
      "Processed dates in 20230606_hanteochart_daily.csv\n",
      "Processed dates in 20230607_hanteochart_daily.csv\n",
      "Processed dates in 20230608_hanteochart_daily.csv\n",
      "Processed dates in 20230609_hanteochart_daily.csv\n",
      "Processed dates in 20230610_hanteochart_daily.csv\n",
      "Processed dates in 20230611_hanteochart_daily.csv\n",
      "Processed dates in 20230612_hanteochart_daily.csv\n",
      "Processed dates in 20230613_hanteochart_daily.csv\n",
      "Processed dates in 20230614_hanteochart_daily.csv\n",
      "Processed dates in 20230615_hanteochart_daily.csv\n",
      "Processed dates in 20230616_hanteochart_daily.csv\n",
      "Processed dates in 20230617_hanteochart_daily.csv\n",
      "Processed dates in 20230618_hanteochart_daily.csv\n",
      "Processed dates in 20230619_hanteochart_daily.csv\n",
      "Processed dates in 20230620_hanteochart_daily.csv\n",
      "Processed dates in 20230621_hanteochart_daily.csv\n",
      "Processed dates in 20230622_hanteochart_daily.csv\n",
      "Processed dates in 20230623_hanteochart_daily.csv\n",
      "Processed dates in 20230624_hanteochart_daily.csv\n",
      "Processed dates in 20230625_hanteochart_daily.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230601.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230602.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230603.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230604.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230605.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230606.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230607.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230608.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230609.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230610.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230611.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230612.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230613.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230614.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230615.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230617.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230618.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230619.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230620.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230621.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230622.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230623.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230624.csv\n",
      "Processed dates in hanteo_year.csv\n",
      "Processed dates in 20230410_melon_boy.csv\n",
      "Processed dates in 20230417_melon_boy.csv\n",
      "Processed dates in 20230418_melon_boy.csv\n",
      "Processed dates in 20230419_melon_boy.csv\n",
      "Processed dates in 20230420_melon_boy.csv\n",
      "Processed dates in 20230421_melon_boy.csv\n",
      "Processed dates in 20230423_melon_boy.csv\n",
      "Processed dates in 20230424_melon_boy.csv\n",
      "Processed dates in 20230425_melon_boy.csv\n",
      "Processed dates in 20230426_melon_boy.csv\n",
      "Processed dates in 20230427_melon_boy.csv\n",
      "Processed dates in 20230428_melon_boy.csv\n",
      "Processed dates in 20230429_melon_boy.csv\n",
      "Processed dates in 20230430_melon_boy.csv\n",
      "Processed dates in 20230501_melon_boy.csv\n",
      "Processed dates in 20230504_melon_boy.csv\n",
      "Processed dates in 20230505_melon_boy.csv\n",
      "Processed dates in 20230506_melon_boy.csv\n",
      "Processed dates in 20230507_melon_boy.csv\n",
      "Processed dates in 20230510_melon_boy.csv\n",
      "Processed dates in 20230512_melon_boy.csv\n",
      "Processed dates in 20230513_melon_boy.csv\n",
      "Processed dates in 20230514_melon_boy.csv\n",
      "Processed dates in 20230515_melon_boy.csv\n",
      "Processed dates in 20230517_melon_boy.csv\n",
      "Processed dates in 20230518_melon_boy.csv\n",
      "Processed dates in 20230519_melon_boy.csv\n",
      "Processed dates in 20230520_melon_boy.csv\n",
      "Processed dates in 20230521_melon_boy.csv\n",
      "Processed dates in 20230522_melon_boy.csv\n",
      "Processed dates in 20230523_melon_boy.csv\n",
      "Processed dates in 20230524_melon_boy.csv\n",
      "Processed dates in 20230525_melon_boy.csv\n",
      "Processed dates in 20230528_melon_boy.csv\n",
      "Processed dates in 20230529_melon_boy.csv\n",
      "Processed dates in 20230530_melon_boy.csv\n",
      "Processed dates in 20230531_melon_boy.csv\n",
      "Processed dates in 20230601_melon_boy.csv\n",
      "Processed dates in 20230602_melon_boy.csv\n",
      "Processed dates in 20230603_melon_boy.csv\n",
      "Processed dates in 20230604_melon_boy.csv\n",
      "Processed dates in 20230605_melon_boy.csv\n",
      "Processed dates in 20230606_melon_boy.csv\n",
      "Processed dates in 20230607_melon_boy.csv\n",
      "Processed dates in 20230608_melon_boy.csv\n",
      "Processed dates in 20230609_melon_boy.csv\n",
      "Processed dates in 20230610_melon_boy.csv\n",
      "Processed dates in 20230611_melon_boy.csv\n",
      "Processed dates in 20230612_melon_boy.csv\n",
      "Processed dates in 20230613_melon_boy.csv\n",
      "Processed dates in 20230614_melon_boy.csv\n",
      "Processed dates in 20230615_melon_boy.csv\n",
      "Processed dates in 20230616_melon_boy.csv\n",
      "Processed dates in 20230617_melon_boy.csv\n",
      "Processed dates in 20230618_melon_boy.csv\n",
      "Processed dates in 20230619_melon_boy.csv\n",
      "Processed dates in 20230620_melon_boy.csv\n",
      "Processed dates in 20230621_melon_boy.csv\n",
      "Processed dates in 20230622_melon_boy.csv\n",
      "Processed dates in 20230623_melon_boy.csv\n",
      "Processed dates in 20230624_melon_boy.csv\n",
      "Processed dates in melon_boy_pre_data_20230417.csv\n",
      "Processed dates in melon_boy_pre_data_20230418.csv\n",
      "Processed dates in melon_boy_pre_data_20230419.csv\n",
      "Processed dates in melon_boy_pre_data_20230420.csv\n",
      "Processed dates in melon_boy_pre_data_20230421.csv\n",
      "Processed dates in melon_boy_pre_data_20230423.csv\n",
      "Processed dates in melon_boy_pre_data_20230424.csv\n",
      "Processed dates in melon_boy_pre_data_20230425.csv\n",
      "Processed dates in melon_boy_pre_data_20230426.csv\n",
      "Processed dates in melon_boy_pre_data_20230427.csv\n",
      "Processed dates in melon_boy_pre_data_20230428.csv\n",
      "Processed dates in melon_boy_pre_data_20230429.csv\n",
      "Processed dates in melon_boy_pre_data_20230430.csv\n",
      "Processed dates in melon_boy_pre_data_20230501.csv\n",
      "Processed dates in melon_boy_pre_data_20230504.csv\n",
      "Processed dates in melon_boy_pre_data_20230505.csv\n",
      "Processed dates in melon_boy_pre_data_20230506.csv\n",
      "Processed dates in melon_boy_pre_data_20230507.csv\n",
      "Processed dates in melon_boy_pre_data_20230510.csv\n",
      "Processed dates in melon_boy_pre_data_20230512.csv\n",
      "Processed dates in melon_boy_pre_data_20230513.csv\n",
      "Processed dates in melon_boy_pre_data_20230514.csv\n",
      "Processed dates in melon_boy_pre_data_20230515.csv\n",
      "Processed dates in melon_boy_pre_data_20230517.csv\n",
      "Processed dates in melon_boy_pre_data_20230518.csv\n",
      "Processed dates in melon_boy_pre_data_20230519.csv\n",
      "Processed dates in melon_boy_pre_data_20230520.csv\n",
      "Processed dates in melon_boy_pre_data_20230521.csv\n",
      "Processed dates in melon_boy_pre_data_20230522.csv\n",
      "Processed dates in melon_boy_pre_data_20230523.csv\n",
      "Processed dates in melon_boy_pre_data_20230524.csv\n",
      "Processed dates in melon_boy_pre_data_20230525.csv\n",
      "Processed dates in melon_boy_pre_data_20230528.csv\n",
      "Processed dates in melon_boy_pre_data_20230529.csv\n",
      "Processed dates in melon_boy_pre_data_20230530.csv\n",
      "Processed dates in melon_boy_pre_data_20230531.csv\n",
      "Processed dates in melon_boy_pre_data_20230601.csv\n",
      "Processed dates in melon_boy_pre_data_20230602.csv\n",
      "Processed dates in melon_boy_pre_data_20230603.csv\n",
      "Processed dates in melon_boy_pre_data_20230604.csv\n",
      "Processed dates in melon_boy_pre_data_20230605.csv\n",
      "Processed dates in melon_boy_pre_data_20230606.csv\n",
      "Processed dates in melon_boy_pre_data_20230607.csv\n",
      "Processed dates in melon_boy_pre_data_20230608.csv\n",
      "Processed dates in melon_boy_pre_data_20230609.csv\n",
      "Processed dates in melon_boy_pre_data_20230610.csv\n",
      "Processed dates in melon_boy_pre_data_20230611.csv\n",
      "Processed dates in melon_boy_pre_data_20230612.csv\n",
      "Processed dates in melon_boy_pre_data_20230613.csv\n",
      "Processed dates in melon_boy_pre_data_20230614.csv\n",
      "Processed dates in melon_boy_pre_data_20230615.csv\n",
      "Processed dates in melon_boy_pre_data_20230616.csv\n",
      "Processed dates in melon_boy_pre_data_20230617.csv\n",
      "Processed dates in melon_boy_pre_data_20230618.csv\n",
      "Processed dates in melon_boy_pre_data_20230619.csv\n",
      "Processed dates in melon_boy_pre_data_20230620.csv\n",
      "Processed dates in melon_boy_pre_data_20230621.csv\n",
      "Processed dates in melon_boy_pre_data_20230622.csv\n",
      "Processed dates in melon_boy_pre_data_20230623.csv\n",
      "Processed dates in melon_boy_pre_data_20230624.csv\n",
      "Processed dates in 20230410_melon_girl.csv\n",
      "Processed dates in 20230417_melon_girl.csv\n",
      "Processed dates in 20230418_melon_girl.csv\n",
      "Processed dates in 20230419_melon_girl.csv\n",
      "Processed dates in 20230420_melon_girl.csv\n",
      "Processed dates in 20230421_melon_girl.csv\n",
      "Processed dates in 20230423_melon_girl.csv\n",
      "Processed dates in 20230424_melon_girl.csv\n",
      "Processed dates in 20230425_melon_girl.csv\n",
      "Processed dates in 20230426_melon_girl.csv\n",
      "Processed dates in 20230427_melon_girl.csv\n",
      "Processed dates in 20230428_melon_girl.csv\n",
      "Processed dates in 20230429_melon_girl.csv\n",
      "Processed dates in 20230430_melon_girl.csv\n",
      "Processed dates in 20230501_melon_girl.csv\n",
      "Processed dates in 20230504_melon_girl.csv\n",
      "Processed dates in 20230505_melon_girl.csv\n",
      "Processed dates in 20230506_melon_girl.csv\n",
      "Processed dates in 20230507_melon_girl.csv\n",
      "Processed dates in 20230510_melon_girl.csv\n",
      "Processed dates in 20230511_melon_girl.csv\n",
      "Processed dates in 20230512_melon_girl.csv\n",
      "Processed dates in 20230513_melon_girl.csv\n",
      "Processed dates in 20230514_melon_girl.csv\n",
      "Processed dates in 20230515_melon_girl.csv\n",
      "Processed dates in 20230517_melon_girl.csv\n",
      "Processed dates in 20230518_melon_girl.csv\n",
      "Processed dates in 20230519_melon_girl.csv\n",
      "Processed dates in 20230520_melon_girl.csv\n",
      "Processed dates in 20230521_melon_girl.csv\n",
      "Processed dates in 20230522_melon_girl.csv\n",
      "Processed dates in 20230523_melon_girl.csv\n",
      "Processed dates in 20230524_melon_girl.csv\n",
      "Processed dates in 20230525_melon_girl.csv\n",
      "Processed dates in 20230528_melon_girl.csv\n",
      "Processed dates in 20230529_melon_girl.csv\n",
      "Processed dates in 20230530_melon_girl.csv\n",
      "Processed dates in 20230531_melon_girl.csv\n",
      "Processed dates in 20230601_melon_girl.csv\n",
      "Processed dates in 20230602_melon_girl.csv\n",
      "Processed dates in 20230603_melon_girl.csv\n",
      "Processed dates in 20230604_melon_girl.csv\n",
      "Processed dates in 20230605_melon_girl.csv\n",
      "Processed dates in 20230606_melon_girl.csv\n",
      "Processed dates in 20230607_melon_girl.csv\n",
      "Processed dates in 20230608_melon_girl.csv\n",
      "Processed dates in 20230609_melon_girl.csv\n",
      "Processed dates in 20230610_melon_girl.csv\n",
      "Processed dates in 20230611_melon_girl.csv\n",
      "Processed dates in 20230612_melon_girl.csv\n",
      "Processed dates in 20230613_melon_girl.csv\n",
      "Processed dates in 20230614_melon_girl.csv\n",
      "Processed dates in 20230615_melon_girl.csv\n",
      "Processed dates in 20230616_melon_girl.csv\n",
      "Processed dates in 20230617_melon_girl.csv\n",
      "Processed dates in 20230618_melon_girl.csv\n",
      "Processed dates in 20230619_melon_girl.csv\n",
      "Processed dates in 20230620_melon_girl.csv\n",
      "Processed dates in 20230621_melon_girl.csv\n",
      "Processed dates in 20230622_melon_girl.csv\n",
      "Processed dates in 20230623_melon_girl.csv\n",
      "Processed dates in 20230624_melon_girl.csv\n",
      "Processed dates in melon_girl_pre_data_20230417.csv\n",
      "Processed dates in melon_girl_pre_data_20230418.csv\n",
      "Processed dates in melon_girl_pre_data_20230419.csv\n",
      "Processed dates in melon_girl_pre_data_20230420.csv\n",
      "Processed dates in melon_girl_pre_data_20230421.csv\n",
      "Processed dates in melon_girl_pre_data_20230423.csv\n",
      "Processed dates in melon_girl_pre_data_20230424.csv\n",
      "Processed dates in melon_girl_pre_data_20230425.csv\n",
      "Processed dates in melon_girl_pre_data_20230426.csv\n",
      "Processed dates in melon_girl_pre_data_20230427.csv\n",
      "Processed dates in melon_girl_pre_data_20230428.csv\n",
      "Processed dates in melon_girl_pre_data_20230429.csv\n",
      "Processed dates in melon_girl_pre_data_20230430.csv\n",
      "Processed dates in melon_girl_pre_data_20230501.csv\n",
      "Processed dates in melon_girl_pre_data_20230504.csv\n",
      "Processed dates in melon_girl_pre_data_20230505.csv\n",
      "Processed dates in melon_girl_pre_data_20230506.csv\n",
      "Processed dates in melon_girl_pre_data_20230507.csv\n",
      "Processed dates in melon_girl_pre_data_20230510.csv\n",
      "Processed dates in melon_girl_pre_data_20230512.csv\n",
      "Processed dates in melon_girl_pre_data_20230513.csv\n",
      "Processed dates in melon_girl_pre_data_20230514.csv\n",
      "Processed dates in melon_girl_pre_data_20230515.csv\n",
      "Processed dates in melon_girl_pre_data_20230517.csv\n",
      "Processed dates in melon_girl_pre_data_20230518.csv\n",
      "Processed dates in melon_girl_pre_data_20230519.csv\n",
      "Processed dates in melon_girl_pre_data_20230520.csv\n",
      "Processed dates in melon_girl_pre_data_20230521.csv\n",
      "Processed dates in melon_girl_pre_data_20230522.csv\n",
      "Processed dates in melon_girl_pre_data_20230523.csv\n",
      "Processed dates in melon_girl_pre_data_20230524.csv\n",
      "Processed dates in melon_girl_pre_data_20230525.csv\n",
      "Processed dates in melon_girl_pre_data_20230528.csv\n",
      "Processed dates in melon_girl_pre_data_20230529.csv\n",
      "Processed dates in melon_girl_pre_data_20230530.csv\n",
      "Processed dates in melon_girl_pre_data_20230531.csv\n",
      "Processed dates in melon_girl_pre_data_20230601.csv\n",
      "Processed dates in melon_girl_pre_data_20230602.csv\n",
      "Processed dates in melon_girl_pre_data_20230603.csv\n",
      "Processed dates in melon_girl_pre_data_20230604.csv\n",
      "Processed dates in melon_girl_pre_data_20230605.csv\n",
      "Processed dates in melon_girl_pre_data_20230606.csv\n",
      "Processed dates in melon_girl_pre_data_20230607.csv\n",
      "Processed dates in melon_girl_pre_data_20230608.csv\n",
      "Processed dates in melon_girl_pre_data_20230609.csv\n",
      "Processed dates in melon_girl_pre_data_20230610.csv\n",
      "Processed dates in melon_girl_pre_data_20230611.csv\n",
      "Processed dates in melon_girl_pre_data_20230612.csv\n",
      "Processed dates in melon_girl_pre_data_20230613.csv\n",
      "Processed dates in melon_girl_pre_data_20230614.csv\n",
      "Processed dates in melon_girl_pre_data_20230615.csv\n",
      "Processed dates in melon_girl_pre_data_20230616.csv\n",
      "Processed dates in melon_girl_pre_data_20230617.csv\n",
      "Processed dates in melon_girl_pre_data_20230618.csv\n",
      "Processed dates in melon_girl_pre_data_20230619.csv\n",
      "Processed dates in melon_girl_pre_data_20230620.csv\n",
      "Processed dates in melon_girl_pre_data_20230621.csv\n",
      "Processed dates in melon_girl_pre_data_20230622.csv\n",
      "Processed dates in melon_girl_pre_data_20230623.csv\n",
      "Processed dates in melon_girl_pre_data_20230624.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230601.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230602.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230603.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230604.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230605.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230606.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230607.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230608.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230609.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230610.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230611.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230612.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230613.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230614.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230615.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230617.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230618.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230619.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230620.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230621.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230622.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230623.csv\n",
      "Processed dates in hanteochart_daily_pre_data_20230624.csv\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xba in position 96: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12484/692122450.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0moutput_directory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/c0206/Desktop/data_encoding_fix'\u001b[0m \u001b[1;31m# 변환된 파일이 저장된 디렉토리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mprocess_dates_in_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12484/692122450.py\u001b[0m in \u001b[0;36mprocess_dates_in_directory\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;31m# Iterate through columns and look for date-like strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xba in position 96: invalid start byte"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        # Try parsing the date in different formats\n",
    "        formats = [\"%Y-%m-%d\", \"%m월 %d일\", \"%Y.%m.%d\"]\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                return datetime.strptime(date_str, fmt).strftime(\"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date: {date_str}, {e}\")\n",
    "    return date_str\n",
    "\n",
    "def process_dates_in_directory(directory):\n",
    "    for subdirectory in subdirectories:\n",
    "        directory = os.path.join('C:/Users/c0206/Desktop/data_encoding_fix', subdirectory)\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Iterate through columns and look for date-like strings\n",
    "                for column in df.columns:\n",
    "                    if df[column].dtype == 'object':\n",
    "                        # Check if the column contains date-like strings\n",
    "                        df[column] = df[column].apply(lambda x: convert_date_format(str(x)) if isinstance(x, str) else x)\n",
    "\n",
    "                # Save the processed DataFrame\n",
    "                df.to_csv(file_path, index=False)\n",
    "                print(f\"Processed dates in {filename}\")\n",
    "\n",
    "def get_subdirectories(directory):\n",
    "    subdirectories = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    return subdirectories\n",
    "\n",
    "directory = 'C:/Users/c0206/Desktop/data' # 원하는 디렉토리 경로\n",
    "subdirectories = get_subdirectories(directory)\n",
    "                \n",
    "\n",
    "output_directory = 'C:/Users/c0206/Desktop/data_encoding_fix' # 변환된 파일이 저장된 디렉토리\n",
    "process_dates_in_directory(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle relative dates like \"2일 전\"\n",
    "def handle_relative_dates(row):\n",
    "    date_str = row['date']\n",
    "    save_time = row['save_time']\n",
    "    \n",
    "    # Handling \"일전\" pattern\n",
    "    if \"일 전\" in date_str:\n",
    "        days_ago = int(date_str.replace(\"일 전\", \"\"))\n",
    "        calculated_date = datetime.strptime(str(int(save_time)), \"%Y%m%d\") - pd.Timedelta(days=days_ago)\n",
    "        return calculated_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Handling \"시간 전\" pattern\n",
    "    elif \"시간 전\" in date_str:\n",
    "        hours_ago = int(date_str.replace(\"시간 전\", \"\"))\n",
    "        calculated_date = datetime.strptime(str(int(save_time)), \"%Y%m%d\") - pd.Timedelta(hours=hours_ago)\n",
    "        return calculated_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "# Applying the conversion to the \"date\" column based on the \"save_time\" column\n",
    "final_date_example_df['date'] = final_date_example_df.apply(handle_relative_dates, axis=1)\n",
    "\n",
    "# Verifying the changes by checking the 21st row (index 20)\n",
    "final_date_example_df.iloc[20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e55584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle relative dates like \"2일 전\"\n",
    "def handle_relative_dates(row):\n",
    "    date_str = row['date']\n",
    "    save_time = row['save_time']\n",
    "    \n",
    "    if \"일 전\" in date_str:\n",
    "        days_ago = int(date_str.replace(\"일 전\", \"\"))\n",
    "        calculated_date = datetime.strptime(str(int(save_time)), \"%Y%m%d\") - pd.Timedelta(days=days_ago)\n",
    "        return calculated_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Handling \"시간 전\" pattern\n",
    "    elif \"시간 전\" in date_str:\n",
    "        hours_ago = int(date_str.replace(\"시간 전\", \"\"))\n",
    "        calculated_date = datetime.strptime(str(int(save_time)), \"%Y%m%d\") - pd.Timedelta(hours=hours_ago)\n",
    "        return calculated_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "\n",
    "# Reading the final file using 'cp949' encoding\n",
    "final_file_path = 'C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok/tiktok_boys_pre_data_20230616.csv'\n",
    "final_date_example_df = pd.read_csv(final_file_path)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to convert the given date format to \"yyyy-mm-dd\"\n",
    "def convert_to_standard_date(date_str):\n",
    "    try:\n",
    "        # Assuming the year is the current year for the conversion\n",
    "        return datetime.strptime(f\"2023-{date_str}\", \"%Y-%m월 %d일\").strftime(\"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        return date_str\n",
    "\n",
    "# Apply the conversion to the \"date\" column\n",
    "final_date_example_df['date'] = final_date_example_df['date'].apply(convert_to_standard_date)\n",
    "\n",
    "\n",
    "# Applying the conversion to the \"date\" column based on the \"save_time\" column\n",
    "final_date_example_df['date'] = final_date_example_df.apply(handle_relative_dates, axis=1)\n",
    "\n",
    "# Replacing NaN values in \"date\" and \"save_time\" columns with -1\n",
    "final_date_example_df['date'].fillna(\"-1\", inplace=True)\n",
    "final_date_example_df['save_time'].fillna(-1, inplace=True)\n",
    "\n",
    "# Save the updated file\n",
    "final_updated_file_path = 'C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/tiktok_boys/20230616_tiktok_boys_updated.csv'\n",
    "final_date_example_df.to_csv(final_updated_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07ad124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to convert date strings like \"2-9\" to standard date format\n",
    "def handle_special_date_format(row):\n",
    "    date_str = str(row['date'])\n",
    "    \n",
    "    if '-' in date_str:\n",
    "        date_elements = date_str.split('-')\n",
    "        month = date_elements[0]\n",
    "        day = date_elements[1]\n",
    "        return f\"2023-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "\n",
    "    return date_str\n",
    "\n",
    "# Function to handle relative dates like \"2일 전\"\n",
    "def handle_relative_dates(row):\n",
    "    date_str = str(row['date'])\n",
    "    save_time = str(int(row['save_time']))\n",
    "    \n",
    "    # Handling \"일 전\" pattern\n",
    "    if \"일 전\" in date_str:\n",
    "        days_ago = int(date_str.replace(\"일 전\", \"\").strip())\n",
    "        calculated_date = datetime.strptime(save_time, \"%Y%m%d\") - pd.Timedelta(days=days_ago)\n",
    "        return calculated_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Handling \"시간 전\" pattern\n",
    "    elif \"시간 전\" in date_str:\n",
    "        hours_ago = int(date_str.replace(\"시간 전\", \"\").strip())\n",
    "        calculated_date = datetime.strptime(save_time, \"%Y%m%d\") - pd.Timedelta(hours=hours_ago)\n",
    "        return calculated_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "# Reading the uploaded file using 'utf-8' encoding\n",
    "file_path = 'C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok/tiktok_boys_pre_data_20230616.csv'\n",
    "final_date_example_uploaded_df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Apply the special date conversion to the \"date\" column\n",
    "final_date_example_uploaded_df['date'] = final_date_example_uploaded_df.apply(handle_special_date_format, axis=1)\n",
    "\n",
    "# Applying the conversion to the \"date\" column based on the \"save_time\" column\n",
    "final_date_example_uploaded_df['date'] = final_date_example_uploaded_df.apply(handle_relative_dates, axis=1)\n",
    "\n",
    "# Replacing NaN values in \"date\" and \"save_time\" columns with -1\n",
    "final_date_example_uploaded_df['date'].fillna(\"-1\", inplace=True)\n",
    "final_date_example_uploaded_df['save_time'].fillna(-1, inplace=True)\n",
    "\n",
    "# Save the updated file\n",
    "final_updated_file_path = 'C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok/tiktok_boys_pre_data_20230616.csv'\n",
    "final_date_example_uploaded_df.to_csv(final_updated_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d7d9c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_boys_pre_data_20230616.csv...\n",
      "Processed C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_boys_pre_data_20230616.csv.\n",
      "Processing C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_boys_pre_data_20230618.csv...\n",
      "Processed C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_boys_pre_data_20230618.csv.\n",
      "Processing C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_boys_pre_data_20230619.csv...\n",
      "Processed C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_boys_pre_data_20230619.csv.\n",
      "Processing C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_boys_pre_data_updated_file.csv...\n",
      "Processed C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_boys_pre_data_updated_file.csv.\n",
      "Processing C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_girls_pre_data_20230616.csv...\n",
      "Processed C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_girls_pre_data_20230616.csv.\n",
      "Processing C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_girls_pre_data_20230618.csv...\n",
      "Processed C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_girls_pre_data_20230618.csv.\n",
      "Processing C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_girls_pre_data_20230619.csv...\n",
      "Processed C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok\\tiktok_girls_pre_data_20230619.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def process_file(file_path):\n",
    "    # Read the file\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "    # Apply the special date conversion to the \"date\" column\n",
    "    df['date'] = df.apply(handle_special_date_format, axis=1)\n",
    "\n",
    "    # Applying the conversion to the \"date\" column based on the \"save_time\" column\n",
    "    df['date'] = df.apply(handle_relative_dates, axis=1)\n",
    "\n",
    "    # Replacing NaN values in \"date\" and \"save_time\" columns with -1\n",
    "    df['date'].fillna(\"-1\", inplace=True)\n",
    "    df['save_time'].fillna(-1, inplace=True)\n",
    "\n",
    "    # Save the updated file\n",
    "    updated_file_path = file_path.replace('.csv', '_updated.csv')\n",
    "    df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "def process_all_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing {file_path}...\")\n",
    "            process_file(file_path)\n",
    "            print(f\"Processed {file_path}.\")\n",
    "\n",
    "directory = 'C:/Users/c0206/Desktop/연구데이터정리/SVIL_DATA_20230624/pre_tiktok'\n",
    "process_all_files(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc3d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
